{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16b34084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecdbff49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/abhaydeep/opt/anaconda3/lib/python3.9/site-packages (3.6.5)\n",
      "Requirement already satisfied: click in /Users/abhaydeep/opt/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /Users/abhaydeep/opt/anaconda3/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/abhaydeep/opt/anaconda3/lib/python3.9/site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in /Users/abhaydeep/opt/anaconda3/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e352835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4c7992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c29738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus : a large collection of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9572ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff3e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9b4a6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c8606bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.sents give sentences of categories\n",
    "data=brown.sents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee6b1e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'was', 'well', 'rid', 'of', 'her', '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=brown.sents(categories=\"adventure\")\n",
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b54a5dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He was well rid of her .'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first setence in the adventure category\n",
    "' '.join(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48775563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90854239",
   "metadata": {},
   "source": [
    "###  Bag of words pipeline\n",
    "1. get the data/corpus\n",
    "2. tokenisation, stopward removal\n",
    "3. stemming\n",
    "4. building a vocab\n",
    "5. vectorization\n",
    "5. classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc8667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4afa81ca",
   "metadata": {},
   "source": [
    "#### tokenisation and stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "279eb2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "document=\"\"\"It was a very pleasant day. The weather was cool and there were light showers. I went to the market\n",
    "to buy some fruits.\"\"\"\n",
    "\n",
    "sentence=\"\"\"Send all the 50 documents related ti chapters 1,2,3 at prateek@cb.com\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4aa568ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eaa19924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It was a very pleasant day.',\n",
       " 'The weather was cool and there were light showers.',\n",
       " 'I went to the market\\nto buy some fruits.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents=sent_tokenize(document)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "061e6442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was a very pleasant day.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "860210c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', '50', 'documents', 'related', 'ti', 'chapters', '1,2,3', 'at', 'prateek', '@', 'cb.com']\n"
     ]
    }
   ],
   "source": [
    "words=word_tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1887ee60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c0e450c",
   "metadata": {},
   "source": [
    "#### stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84b5d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2b80883",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'some', 'doesn', 'mustn', 'below', 'haven', 'as', 'here', 'ain', 're', 'and', 'are', 'shan', \"needn't\", 'he', 'when', \"shan't\", 'weren', \"she's\", 'were', 'has', 'doing', 've', 'had', \"you'd\", 'am', 'where', \"you'll\", 'is', 'themselves', 'there', 'your', 'her', 'their', 'a', 'our', 'each', \"won't\", 'hadn', 'too', 'was', 'himself', 'nor', \"mightn't\", 'o', 'up', 'them', 's', 'don', 'hasn', 'wouldn', 'didn', 'yours', 'which', 'once', 'over', \"you're\", \"wasn't\", \"aren't\", 'down', \"don't\", \"didn't\", 'needn', 'more', 'll', 'will', \"you've\", 'yourselves', 'above', 'no', 'because', 'against', 'my', \"that'll\", 'but', 'him', 'd', \"weren't\", 't', 'how', 'about', 'then', \"should've\", 'if', 'between', 'his', \"hasn't\", \"shouldn't\", 'being', 'they', 'did', 'until', 'we', 'further', 'shouldn', 'with', \"wouldn't\", 'after', 'most', 'hers', 'myself', 'herself', 'those', 'or', 'other', 'having', 'these', 'before', 'off', 'during', 'been', 'wasn', 'i', 'itself', 'any', \"mustn't\", \"hadn't\", 'have', 'while', 'its', 'm', 'by', 'under', 'to', 'same', 'the', 'such', 'y', 'through', 'yourself', 'again', \"doesn't\", 'all', 'aren', 'just', 'what', 'mightn', \"it's\", 'can', 'theirs', 'should', 'ourselves', 'does', \"isn't\", 'isn', 'ours', 'at', 'so', 'couldn', 'that', 'now', 'an', 'in', 'ma', 'own', 'do', 'be', 'very', 'than', 'not', 'of', 'into', 'on', 'whom', 'few', 'from', 'for', 'me', 'only', \"haven't\", \"couldn't\", 'this', 'why', 'out', 'she', 'both', 'who', 'it', 'won', 'you'}\n"
     ]
    }
   ],
   "source": [
    "sw=set(stopwords.words('english'))\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6653d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea971fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(text,stopwords):\n",
    "    useful_words=[w for w in text if w not in stopwords]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87ae7b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "document2=\"\"\"It was a very pleasant day. The weather was cool and there were light showers. I went to the market\n",
    "to buy some fruits.\"\"\"\n",
    "# since its a string and we want list to iterate words \n",
    "wordd=word_tokenize(document2)\n",
    "\n",
    "useful_text=removeStopwords(wordd, sw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d34b88cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'pleasant', 'day', '.', 'The', 'weather', 'cool', 'light', 'showers', '.', 'I', 'went', 'market', 'buy', 'fruits', '.']\n"
     ]
    }
   ],
   "source": [
    "print(useful_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d945854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glitch we have not in stopwords which can affect our sentence sentiment\n",
    "# so we can remove not from stopwords manually or can create our own stopwords\n",
    "'not' in sw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b7d5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de9105c8",
   "metadata": {},
   "source": [
    "####  tokenization using regular expressions \n",
    "creating our own logic to break our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "afde340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not satified with custom tokenizer we can make our own using regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "53921b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "577efceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a-z , A-Z @ .  1-9  select and + means one or more occurenece of preceding characters\n",
    "tokenizer=RegexpTokenizer('[a-zA-Z@.1-9]+')\n",
    "useful_text=tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f08aed66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', '5', 'documents', 'related', 'ti', 'chapters', '1', '2', '3', 'at', 'prateek@cb.com']\n"
     ]
    }
   ],
   "source": [
    "print(useful_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d1f42853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can take help from regexpal.com for more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394c2336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5cb6848",
   "metadata": {},
   "source": [
    "#### stemming\n",
    "1. Process that transform particular words (verbs, plurals) into their radical form \n",
    "2. Preserve the semantics of the sentence without increasing the number if unique tokens\n",
    "3. Example -jumps,jumping, jumped ,jump ==>  jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60048d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Foxes love to make jumps.\n",
    "The quick brown fox was seen jumping over the lovely dog from a 6ft feet high wall\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0fb7b285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk provides 3 types of stemmer Snowball stemmer, porter,Lancaster, stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "21135b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer, PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c4a28eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "366bfa73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "284d9d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('jump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3de2749e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## SnowballStemmer supports multiple language like english spanish, french\n",
    "ss=SnowballStemmer('english')\n",
    "ss.stem('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d32c2590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization , same as stemming\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ae648589",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has jumps\n",
      "jumped\n",
      "jumping\n"
     ]
    }
   ],
   "source": [
    "# maintain the past, present and future tense\n",
    "wn=WordNetLemmatizer()\n",
    "print(wn.lemmatize('jumps'))\n",
    "print(wn.lemmatize('jumped'))\n",
    "print(wn.lemmatize('jumping'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbc6763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1a1d138",
   "metadata": {},
   "source": [
    "### building a vocab & Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ab5c985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample corpus - contains 4 documents, each document can have 1 or more sentences\n",
    "corpus=[\n",
    "    'Indian cricket team will wins wins World Cup, says Capt.Virat Kahli.',\n",
    "    'We will win next Lik Sabha Elections, says confident Indian PM',\n",
    "    'The nobel laurate won the hearts of the People',\n",
    "    'The movie razi is an exiting Indian Spy thriller movie based on real story'\n",
    "]\n",
    "# 4 document therfore 4 feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "04b4fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can do directly by iterating over text and maintain dictonary\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b7ace4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "63755dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5f527a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 2 0 1]\n"
     ]
    }
   ],
   "source": [
    "vectorized_corpus=vectorized_corpus.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b621f36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 2 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_corpus[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "04a07d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indian': 9,\n",
       " 'cricket': 4,\n",
       " 'team': 27,\n",
       " 'will': 32,\n",
       " 'wins': 34,\n",
       " 'world': 36,\n",
       " 'cup': 5,\n",
       " 'says': 24,\n",
       " 'capt': 2,\n",
       " 'virat': 30,\n",
       " 'kahli': 11,\n",
       " 'we': 31,\n",
       " 'win': 33,\n",
       " 'next': 15,\n",
       " 'lik': 13,\n",
       " 'sabha': 23,\n",
       " 'elections': 6,\n",
       " 'confident': 3,\n",
       " 'pm': 20,\n",
       " 'the': 28,\n",
       " 'nobel': 16,\n",
       " 'laurate': 12,\n",
       " 'won': 35,\n",
       " 'hearts': 8,\n",
       " 'of': 17,\n",
       " 'people': 19,\n",
       " 'movie': 14,\n",
       " 'razi': 21,\n",
       " 'is': 10,\n",
       " 'an': 0,\n",
       " 'exiting': 7,\n",
       " 'spy': 25,\n",
       " 'thriller': 29,\n",
       " 'based': 1,\n",
       " 'on': 18,\n",
       " 'real': 22,\n",
       " 'story': 26}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2024bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b57cdc6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['indian', 'cricket', 'team', 'will', 'wins', 'world', 'cup',\n",
       "        'says', 'capt', 'virat', 'kahli'], dtype='<U9')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reverse mapping\n",
    "s=cv.fit_transform(corpus)[0]\n",
    "cv.inverse_transform(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a6aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "751f9024",
   "metadata": {},
   "source": [
    "### vectorisation with stopward removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cb6c8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myTokenizer(document):\n",
    "    words=tokenizer.tokenize(document.lower())\n",
    "    \n",
    "    # remove stopwords\n",
    "    words=removeStopwords(words,sw)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ef76516c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['send',\n",
       " '5',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'ti',\n",
       " 'chapters',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " 'prateek@cb.com']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#myTokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6ab2ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(tokenizer=myTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "57b7775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corups=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c1cf1c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorized_corups[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8916093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test data\n",
    "test_corpus=['Indian cricket rock!',]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7533a5e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dont call fit transform for test data , as it override our own vocab\n",
    "# use transform for test and fit_transform for train\n",
    "cv.transform(test_corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7373a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0eb7de68",
   "metadata": {},
   "source": [
    "### more ways to create features\n",
    "1. unigram - every word as a feature\n",
    "2. biagram- two conjective words can be consider as feature\n",
    "3. trigram -\n",
    "4. n-grams\n",
    "5. TF-IDF Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f09732e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1=[\"this is a good movie\"]\n",
    "sent2=[\"this is not good movie\"] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0a982944",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0f3eb204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=[sent1[0],sent2[0]]\n",
    "cv.fit_transform(docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "03671d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this is': 4, 'is good': 1, 'good movie': 0, 'is not': 2, 'not good': 3}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a598c0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do both uni and bi gram\n",
    "cv=CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f0b5d6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0, 1, 0, 0, 1, 1],\n",
       "       [1, 1, 1, 0, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=[sent1[0],sent2[0]]\n",
    "cv.fit_transform(docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0927dafa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 8,\n",
       " 'is': 2,\n",
       " 'good': 0,\n",
       " 'movie': 5,\n",
       " 'this is': 9,\n",
       " 'is good': 3,\n",
       " 'good movie': 1,\n",
       " 'not': 6,\n",
       " 'is not': 4,\n",
       " 'not good': 7}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7978e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2df95281",
   "metadata": {},
   "source": [
    "### Tf-idf Normalisation (term frequency - inverse document freq)\n",
    "1. avoid features that occur very often, because they contain less information\n",
    "2. information decrease as the number of occurences increases across different type of document\n",
    "3. so we define another term - term-document-frequency which associates a weight with every term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7bef58b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen1=\"this is a good movie\"\n",
    "sen2=\"this is not good movie\"\n",
    "sen3=\"this was not good movie\"\n",
    "corpus=[sen1, sen2 , sen3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e989e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a6313710",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6be44de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc=tfidf.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "27cf5168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46333427 0.59662724 0.46333427 0.         0.46333427 0.        ]\n",
      " [0.3978967  0.51236445 0.3978967  0.51236445 0.3978967  0.        ]\n",
      " [0.3645444  0.         0.3645444  0.46941728 0.3645444  0.61722732]]\n"
     ]
    }
   ],
   "source": [
    "print(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fa0dd908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 4, 'is': 1, 'good': 0, 'movie': 2, 'not': 3, 'was': 5}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d4b760",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
